# -*- coding: utf-8 -*-
"""Master_Project_dataset_handling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N3AM9ibVTp3lHd7sUU1QKDIDjKizw6FY

# **Dataset Handling**

This code is made to modify the dataset from Amazon Review.

The code does the following:
*   Remove all records with missing values in both datasets
*   Drop unused featues
*   Remove all records of users with less than 3 reviews
*   Compressing text data into numerical representations
*   Create new datasets

Users with less than 3 reviews will significantly increse the computational cost of the program, barely contributing to the overall result, thus removing them is necessary.

**This code does not contain the entire process of dataset handling. Due to memory litimations it was decided to divide data handling into 2 parts**
"""

# !pip install textblob
# !pip install tqdm

import json
import pandas as pd
import numpy as np
from textblob import TextBlob
from collections import Counter
import string
import gc
from tqdm import tqdm

"""## Reading dataset"""

metadata_path = "../datasets/metadata.jsonl"
reviews_path = "../datasets/reviews.jsonl"

def load_jsonl_to_dataframe(file_path):
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            data.append(json.loads(line.strip()))
    return pd.DataFrame(data)

user_review_df = load_jsonl_to_dataframe(reviews_path)
item_metadata_df = load_jsonl_to_dataframe(metadata_path)

print("User Review Columns:", user_review_df.columns)
print("Item Metadata Columns:", item_metadata_df.columns)

user_review_df[0:5]

item_metadata_df[0:5]

"""## Dropping unused features

These features were dropped because we do not have enough computatial memory and power. It does not mean those featues are useless.
"""

# 1. Drop columns from `user_reviews`
columns_to_drop_user_reviews = ['images', 'asin', 'timestamp', 'helpful_vote', 'verified_purchase']
user_review_df = user_review_df.drop(columns=columns_to_drop_user_reviews, errors='ignore')

# 2. Drop columns from `item_metadata`
columns_to_drop_item_metadata = ['main_category', 'price', 'videos', 'store', 'bought_together', 'subtitle', 'author']
item_metadata_df = item_metadata_df.drop(columns=columns_to_drop_item_metadata, errors='ignore')

user_review_df[0:5]

item_metadata_df[0:5]

"""## Removing users with < 3 reviews"""

# Count all users with more than 3 reviews
user_review_counts = user_review_df['user_id'].value_counts()  # Count reviews per user
users_with_more_than_3_reviews = user_review_counts[user_review_counts >= 3].count()

# Print the result
print(f"Number of users with more than 3 reviews: {users_with_more_than_3_reviews}")

"""Removing all the users with less than 3 reviews"""

# Identify users with 3 or more reviews
valid_users = user_review_counts[user_review_counts >= 3].index

# Filter the dataset to keep only rows where 'user_id' is in valid_users
filtered_user_reviews_df = user_review_df[user_review_df['user_id'].isin(valid_users)]

# Print the result
print(f"Original number of users: {len(user_review_counts)}")
print(f"Number of users with 3 or more reviews: {len(valid_users)}")
print(f"Filtered dataset size: {len(filtered_user_reviews_df)} rows")

"""Removing all item metadata that do not have reviews on them"""

# Get the list of parent_asin values that have at least one review in filtered_user_reviews_df
valid_items = filtered_user_reviews_df['parent_asin'].unique()

# Filter the item_metadata dataset to keep only rows where 'parent_asin' is in valid_items
filtered_item_metadata_df = item_metadata_df[item_metadata_df['parent_asin'].isin(valid_items)]

# Print the result
print(f"Original number of items in metadata: {len(item_metadata_df)}")
print(f"Number of items with reviews: {len(filtered_item_metadata_df)}")

"""## Transforming features

1.  Extracting ***rated*** and ***manufacturer*** from ***details*** and adding them as featurues
2.  Removing ***detailes*** from featues completely
3.  Split arrays in ***categories*** and create columns for the elements of the array
4.  Dtop column ***rating_number***
5.  Removing ***features*** that appear only once in the dataset
6.  Mapping ***features*** to unique id, extracting the table of faeture values and respective IDs, replacing features in dataset with respective IDs
7.  Removing all ***images*** that do not contain "MAIN" variant. If main variant does not exist choose any other.
"""

# Extract 'Rated' and 'Manufacturer' fields from the 'details' column
filtered_item_metadata_df['rated'] = filtered_item_metadata_df['details'].apply(lambda x: x.get('Rated') if isinstance(x, dict) else None)
filtered_item_metadata_df['manufacturer'] = filtered_item_metadata_df['details'].apply(lambda x: x.get('Manufacturer') if isinstance(x, dict) else None)

filtered_item_metadata_df[0:5]

len(filtered_item_metadata_df)

filtered_transformed_item_metadata_df = filtered_item_metadata_df.drop(columns=['details'])
filtered_transformed_item_metadata_df[0:5]

# Count the number of elements in each category list
category_lengths = filtered_transformed_item_metadata_df['categories'].apply(lambda x: len(x) if isinstance(x, list) else 0)

# Calculate the frequency of each length
length_distribution = category_lengths.value_counts().sort_index()

# Calculate the probability distribution
probability_distribution = length_distribution / length_distribution.sum()

# Combine the length and probability into a DataFrame for clarity
distribution_df = pd.DataFrame({
    'Number of Elements': length_distribution.index,
    'Frequency': length_distribution.values,
    'Probability': probability_distribution.values
})

# Display the result
print(distribution_df)

"""In order to split ***categories*** column into multiple columns we need to find an optimal number of columns to split the data into. According to stats above the max number of items in ***categories*** column is 8. However, we can see that the probability of facing >= 6 items is 11%. Thus, in order to not overload the dataset and later the graphs we will only create 5 new columns for the items in ***categories*** column."""

# Create new features (category_1, category_2, ..., category_n)
for i in range(1, 6):  # Dynamically adjust based on max elements
    filtered_transformed_item_metadata_df[f'category_{i}'] = filtered_transformed_item_metadata_df['categories'].apply(
        lambda x: x[i - 1] if isinstance(x, list) and len(x) >= i else None
    )

filtered_transformed_item_metadata_df = filtered_transformed_item_metadata_df.drop(columns=['categories'])
filtered_transformed_item_metadata_df[0:3]

filtered_transformed_item_metadata_df = filtered_transformed_item_metadata_df.drop(columns=['rating_number'])
filtered_transformed_item_metadata_df[0:3]

user_review_df = None
del user_review_df
gc.collect()

"""Selecting all features that appear only once"""

# Step 1: Flatten features to a single list
all_features = [feature for features in filtered_transformed_item_metadata_df['features'] for feature in features]

# Step 2: Count occurrences of each feature
feature_counts = Counter(all_features)

# Step 3: Find features that appear only once
features_appearing_once = [feature for feature, count in feature_counts.items() if count == 1]
features_appearing_once

"""Deleting all features that appear only once, because they will not be useful in graph construction."""

tqdm.pandas(desc="Filtering Features")

filtered_transformed_item_metadata_df['filtered_features'] = filtered_transformed_item_metadata_df['features'].progress_apply(
    lambda features: [feature for feature in features if feature not in features_appearing_once]
)

"""Map remaining features to unique IDs"""

# Create a mapping of feature -> unique ID
remaining_features = {feature for feature in all_features if feature not in features_appearing_once}
feature_to_id_mapping = {feature: idx for idx, feature in enumerate(remaining_features, start=1)}

"""Replace features in the dataset with their respective IDs"""

filtered_transformed_item_metadata_df['mapped_features'] = filtered_transformed_item_metadata_df['filtered_features'].progress_apply(
    lambda features: [feature_to_id_mapping[feature] for feature in features]
)

feature_to_id_mapping

feature_to_id_mapping_df = pd.DataFrame(list(feature_to_id_mapping.items()), columns=['Feature', 'ID'])

feature_to_id_mapping_df[0:3]

filtered_transformed_item_metadata_df = filtered_transformed_item_metadata_df.drop(columns=['features', 'filtered_features'])
filtered_transformed_item_metadata_df[0:3]

"""In the faeture list we can see that the value for the feature with the ID 1 is actually *empty string*, so we must remove it from the dataset and mapping table"""

filtered_transformed_item_metadata_df['mapped_features'] = filtered_transformed_item_metadata_df['mapped_features'].progress_apply(
    lambda features: [feature_id for feature_id in features if feature_id != 1]
)

#Delete feature with ID=1 from feature_to_id_mapping_df
feature_to_id_mapping_df = feature_to_id_mapping_df[feature_to_id_mapping_df['ID'] != 1]

feature_to_id_mapping_df[0:3]

filtered_transformed_item_metadata_df[0:3]

"""Filtering images"""

# Function to filter images column
def filter_main_variant(images):
    if not images:  # Handle cases where images is empty or None
        return []

    # Check for "MAIN" variant
    main_variants = [img for img in images if img.get("variant") == "MAIN"]

    if main_variants:
        return main_variants

    # If no "MAIN" variant exists, take any other variant
    return [images[0]] if images else []

# Apply the filter to the images column
filtered_transformed_item_metadata_df['images'] = filtered_transformed_item_metadata_df['images'].apply(filter_main_variant)

"""## Compressing text data

Compressing text data into numerical representations like embeddings or sentiment scores is a common technique in machine learning to make text manageable and efficient for models.

**What It Means**
Compressing text means converting text fields (like reviews or descriptions) into smaller, structured numerical representations that capture the essential meaning or features of the text.

**Sentiment analysis**
Is to compute a sentiment score for the text (e.g., polarity score from -1 to 1).

Example:
Input: "These were lightweight and soft but much too small for my liking."
Output: Sentiment Score: -0.3 (negative sentiment).
"""

# Function to calculate sentiment polarity
def get_sentiment(text):
    if pd.isna(text):  # Handle missing values
        return None
    return TextBlob(text).sentiment.polarity

# Apply sentiment analysis to 'title' and 'text' columns
filtered_user_reviews_df['title_sentiment'] = filtered_user_reviews_df['title'].apply(get_sentiment)
filtered_user_reviews_df['text_sentiment'] = filtered_user_reviews_df['text'].apply(get_sentiment)

filtered_user_reviews_df[0:5]

filtered_user_reviews_df[5:40]

len(filtered_user_reviews_df)

filtered_user_reviews_df = filtered_user_reviews_df.drop(columns=['title', 'text'])
filtered_user_reviews_df[0:5]

"""## Saving the updated datasets

***User Review Dataframe***
"""

filtered_user_reviews_df.to_csv("../datasets/reviews_new.csv", index=False)

"""Saving ***item metadata*** dataset and feature to id mapping"""

filtered_transformed_item_metadata_df.to_csv("../datasets/metadata_new_1.csv", index=False)
feature_to_id_mapping_df.to_csv('feature_to_id_mapping.csv', index=False)